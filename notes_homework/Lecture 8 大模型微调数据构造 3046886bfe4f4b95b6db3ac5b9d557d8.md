# Lecture 8: 大模型微调数据构造

---

## 笔记

- gpt assistant training pipeline

![Untitled](Lecture%208%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%95%B0%E6%8D%AE%E6%9E%84%E9%80%A0%203046886bfe4f4b95b6db3ac5b9d557d8/Untitled.png)

- 微调
    - 数据：质量好的问答对
    - 目的
        - 预训练模型无法满足需求时
        - prompt engineering 的局限
            - 输入过长、推理成本高、效果不理想
        - 数据安全
        - 需要个性化服务
    - 效果
        - 提升模型性能：减少幻觉，遵循指令，面向特定任务场景
        - 提高数据安全：私有化部署，保护数据资产，防止隐私泄漏
        - 更加经济：降低训练成本、提高训练效果、动态学习能力
    - 微调的步骤
        1. 明确目标任务
        2. 选择和测试基座模型（可以先选一个小模型进行验证）
        3. 数据准备（收集、清洗、预处理、标注、划分）
        4. 设定微调策略（LoRA, QLoRA）
        5. 设置超参（学习率、批量大小、训练轮数）
        6. 模型初始化
        7. 开始微调训练
        8. 模型评估和调优（调整数据、策略、超参）
        9. 模型性能测试
        10. 模型部署应用
    - 好的微调数据
        - 数据质量高
        - 多样性
        - 真实数据
        - 数据量多
    - 微调结果评测
        - 人工评价：更可靠
        - 评测工具：如 OpenCompass，更客观
        - ELO 排名：简单直接

## 作业

所用的数据是豆瓣的图书推荐数据，[来源](https://opendatalab.org.cn/OpenDataLab/COIG-CQIA)

微调前模型的表现，无法回答关于《生死学十四讲》的问题

![Untitled](Lecture%208%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%95%B0%E6%8D%AE%E6%9E%84%E9%80%A0%203046886bfe4f4b95b6db3ac5b9d557d8/Untitled%201.png)

微调后的结果

![Untitled](Lecture%208%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%95%B0%E6%8D%AE%E6%9E%84%E9%80%A0%203046886bfe4f4b95b6db3ac5b9d557d8/Untitled%202.png)